---
# ansible-playbook -v 002.setup_glusterfs_infra.yml -u root
- hosts: all
  become: yes
  become_user: root
  gather_facts: yes

  pre_tasks:
     - name: Update repositories cache and install "xfsprogs" package
       package:
          name: xfsprogs
          update_cache: yes
          state: present

     - name: Ensure that the lvm packages are installed on your target system
       package:
          name: lvm2
          update_cache: yes
          state: present


     - name: "Umount old brick mount point"
       shell: "umount -lf {{gluster_cluster_brick_mount_point}}"
       ignore_errors: True

     - name: "vgremove firefly"
       shell: "vgremove -ff firefly"
       ignore_errors: True

     - name: "pvremove loop"
       shell: "pvremove -ff {{device_path}}"
       ignore_errors: True
     - name: "dmsetup remove_all"
       shell: "dmsetup remove_all"
       ignore_errors: True

     - name: "Detach Loop device"
       shell: "losetup -d {{device_path}}"
       ignore_errors: True

     - name: "Remove /gfs0"
       shell: "rm -f /gfs0"
       ignore_errors: True

     - name: "dmsetup remove_all"
       shell: "dmsetup remove_all"
       ignore_errors: True

     - name: "Remove Loop Device"
       shell: "rm -f {{device_path}}"
       ignore_errors: True

     - name: "Create Loop Device"
       shell: "mknod -m660 {{device_path}} b 7 8"
       ignore_errors: True

     - name: "Set Loop Device Ownership"
       shell: "chown root:disk {{device_path}}"
       ignore_errors: True

    #  - name: "Check and Create a loop device for glusterfs"
    #    shell: "rm -f {{device_path}} && sudo mknod -m660 {{device_path}} b 7 8 && sudo chown root:disk {{device_path}}"

    #  - name: "Remove old setup"
    #    shell: "umount /mnt/brick1 && vgremove -f firefly && pvremove -f /dev/{{device_path}} && losetup -d /dev/{{device_path}} && rm -f /gfs0 && dmsetup remove_all"

     # This section will execute when "devmode=True"
     - name: "Prep the FS in devmode"
       shell: "{{item}}"
       loop:
        - "fallocate -l {{gfs_size}} /gfs0"
        - "losetup {{device_path}} /gfs0"        
       when: devmode
    
     - name: "Finalize the FS"
       shell: "{{item}}"
       loop:
        - "pvcreate {{device_path}}"
        - "vgcreate {{vg_name}} {{device_path}}"
        #- "mkfs.ext4 -F {{device_path}}"

  # See https://github.com/gluster/gluster-ansible-infra for details
  vars:
    # Set a disk type, Options: JBOD, RAID6, RAID10
     gluster_infra_disktype: JBOD
    #  # RAID6 and RAID10 diskcount (Needed only when disktype is raid)
    #  gluster_infra_diskcount: 3
    #  # Stripe unit size always in KiB. gluster_infra_diskcount * gluster_infra_stripe_unit_size should be between 1 - 2 MB
    #  gluster_infra_stripe_unit_size: 384

    # enable lvm auto-extend feature so that when the pool is at 70% it will be extended with 15%
    #  gluster_infra_lvm: {
    #   autoexpand_threshold: 70,
    #   autoexpand_percentage: 15,
    #  }
   
     # enable fstrim service so the TRIM command is executed once in a while to clean either ssd or thin/vdo volumes
     # Have issues with Debian systems. OK with RadHat system
     fstrim_service: {
       enabled: yes,
       schedule: {         
         hour: "{{ range(1, 4) | random() }}"
       }
      }

     gluster_infra_volume_groups:
       - { vgname: 'firefly', pvname: "{{device_path}}" }
     
     # Create thick logical volume
     gluster_infra_thick_lvs:
       - { vgname: 'firefly', lvname: 'thick_lv_1', shrink: no }      

     # Mount the devices
     gluster_infra_mount_devices:
       - { path: '{{gluster_cluster_brick_mount_point}}', vgname: 'firefly', lvname: 'thick_lv_1' }       

  roles:
     - gluster.infra